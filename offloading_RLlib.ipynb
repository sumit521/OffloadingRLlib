{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "#ray.shutdown()\n",
    "ray.init()\n",
    "from ray.rllib.agents.sac import SACTrainer \n",
    "from ray.rllib.agents.ppo import PPOTrainer \n",
    "from ray.rllib.agents.ddpg import TD3Trainer \n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import random\n",
    "SEED = 3333\n",
    "np.random.seed(SEED )\n",
    "random.seed(SEED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PMAX = 2 # Maximum transmitted power by the UE is 2 Watts \n",
    "# We are assuming that the UE has total power which can be splitted between local processing or computation offloading.\n",
    "KAPPA = 1e-27\n",
    "NOISE = 1e-9\n",
    "BETA = 500\n",
    "WD = 2 # weight in utility function for local procesing \n",
    "WE = 1e6\n",
    "t = 0\n",
    "T = 1e-3 #slot duration\n",
    "T_off = 0.4*T #offloading duration is some fraction of the total \n",
    "EPISODE_LENGTH = 200 # 100 steps in every episode. No specific reason for this value.\n",
    "class UE:\n",
    "    def __init__(self,ID = '0x00',mode = 0, Asso_AP_ID = '1x00', P = PMAX, kappa = KAPPA, beta = BETA, wd = WD,we = WE ):\n",
    "        self.ID = ID # string id of a UE\n",
    "        self.mode = []\n",
    "        #self.mode = [np.random.randint(0,2)]\n",
    "        self.alpha = [] # the fraction of power to be spent on offloading of bits\n",
    "        self.Asso_AP_ID = Asso_AP_ID\n",
    "        self.P = P\n",
    "        self.kappa = kappa\n",
    "        self.lambda_ = 5*(1e6)*T # for determining offered offloading. Used in poisson process.\n",
    "        self.task_buffer = 0        \n",
    "        # several lists to store the information regarding the system state at each decision epoch.\n",
    "        self.f = [] # local computing cycles/s\n",
    "        self.offered_offloading = []\n",
    "        self.tx_rate = []\n",
    "        self.drop = []\n",
    "        self.E_l = [] \n",
    "        self.E_o = []\n",
    "        self.D_l = []\n",
    "        self.D_o = []\n",
    "        self.U_l = []\n",
    "        self.U_o = []\n",
    "        self.U = []        \n",
    "        self.beta = beta       \n",
    "        self.wd = WD\n",
    "        self.we = WE        \n",
    "        self.F  = (self.P/self.kappa)**(1/3)\n",
    "\n",
    "\n",
    "\n",
    "    def generate_task(self): # generate a task in a time slot.\n",
    "        #for now we assume that the UE always has a task to send. i.e. task arrival rate has been set very high.\n",
    "        self.task_buffer += (np.random.poisson(self.lambda_))\n",
    "        \n",
    "    \n",
    "    def create_alpha(self): # decide on power allocation between local processing and offloading. This will be predicted by actor network in the actual implelementation.\n",
    "        #Here the function has been included for trial purpose only.\n",
    "         self.alpha.append(np.random.rand())\n",
    "         self.mode.append(np.random.randint(0,2))\n",
    "         self.f.append(((1-self.alpha[t])*self.P/self.kappa)**(1/3))\n",
    "    \n",
    "    def send_offloading_tasks(self):\n",
    "        # select an offloading mode # can be modified include to selection of F-AP also.for now we stick with a basic model\n",
    "        # select Power fraction level\n",
    "        # returns the amount of offered bits for execution to the F-AP\n",
    "        #  self.mode[t] = np.random.randint(0,2)        \n",
    "        # the offered task and the executed task locally should not exceed the buffer size\n",
    "        self.offered_offloading.append(self.tx_rate[t]*T_off*BETA) # offered offloading is in cpu cycles to be executed per second and not in bits\n",
    "        return (self.ID, self.offered_offloading[t])\n",
    "        \n",
    "\n",
    "    def local_execution(self):\n",
    "        self.E_l.append(self.P*(1-self.alpha[t])*T) # local processing energy consumption\n",
    "        self.D_l.append( T*self.f[t]/self.beta) # number of locally processed bits\n",
    "        self.U_l.append(self.wd*self.D_l[t] - self.we*self.E_l[t]) # Utility in local processing\n",
    "        return self.U_l[t]\n",
    "\n",
    "    def offloading_execution(self):\n",
    "        # data transmission rate will be provided by the F-AP node with which it is connected\n",
    "        # drop wil indicate whether the F-AP could process the task or not\n",
    "        self.D_o.append(self.tx_rate[t]*T_off*(1-self.drop[t])) # Number of offloaded bits.        \n",
    "        self.E_o.append(self.alpha[t]*self.P*T_off) # Energy consumed in offloading bits\n",
    "        self.U_o.append(self.wd*self.D_o[t] - self.we*self.E_o[t]) # Utility in offloading bits\n",
    "        return self.U_o[t]\n",
    "\n",
    "    def calculate_utility(self):\n",
    "        self.U.append(self.local_execution() + self.offloading_execution()) # sum of local an offloading utility\n",
    "        self.task_buffer = self.task_buffer - self.D_l[t] - self.D_o[t] # update the task buffer based on local and edge execution \n",
    "        self.task_buffer = max(0,self.task_buffer)    \n",
    "        return self.U[t]\n",
    "\n",
    "\n",
    "class FAP:\n",
    "    # A class for Fog Access point  \n",
    "    def __init__(self,x_pos=250, y_pos=0,ID = '1x00', queue_len =100):\n",
    "        self.cpu = 1.2*1e9 # Available Computational capacity of MEC server\n",
    "        self.ID = ID #ID of F-AP. Currently, using only one F-AP, hence irrelevant\n",
    "        self.B = 1e6 # bandwidth 1MHz\n",
    "        self.v = 1.3 # communication overhead\n",
    "        self.UEs_asso = [] # list of UEs associated with the F-AP\n",
    "        self.possible_channel_gains = dict(zip(range(6),10**(np.array([-11.23,-9.37,-7.8,-6.3,-4.68,-2.08])/10))) # set of 6 possible chains taken from https://ieeexplore.ieee.org/document/8771176\n",
    "        self.reverse_channel_gain_dict = dict((v,k) for k,v in self.possible_channel_gains.items()) # a dictionary to find channel number from gain value        \n",
    "        self.noise = NOISE # Noise power\n",
    "        self.tx_rate = [] # a list to store offered tx rates to UEs in various time slots.\n",
    "        self.capacity = queue_len # how may task bits at max it can store\n",
    "        self.queue = deque([], maxlen = self.capacity)\n",
    "        self.dropped_task = [] # details of dropped task at every slot containing information about ue id and its corresponding bits\n",
    "        self.drop = [{}] # details of dropped task at every slot containing information about ue id and whether task is dropped? (True or false)\n",
    "        self.queue_delay = 0 # not used\n",
    "        self.x = x_pos # positions value not used\n",
    "        self.y = y_pos \n",
    "        self.tasks = []  # a list containing details of task arrivals every time slot       \n",
    "        N = len(self.possible_channel_gains) \n",
    "        transition_matrix = np.random.rand(N,N) # transition matrix for determining the probability of change of channel gains with every time slot\n",
    "        transition_matrix /= transition_matrix.sum(axis =1, keepdims=1) # for converting rand values to probabilitues              \n",
    "        self.transition_matrix = pd.DataFrame(transition_matrix,columns=self.possible_channel_gains.keys(), index=self.possible_channel_gains.keys())\n",
    "\n",
    "\n",
    "    def get_offloading_request(self,*args):\n",
    "    # here elements of args are UE.send_offloading_request()   \n",
    "    # if the sum of offloading bits divided by cpu rate doesn't execeed the task duration then send the drop as false to every UE\n",
    "    # otherwise remove the least requirement size request and again compute the result and so on till the tasks can be executed during the slot duration\n",
    "    # send the drop indicator as True to the UEs whose tasks have been discarded.\n",
    "        self.tasks.append(dict(list(args)))\n",
    "        self.drop.append({})    # create an empty dictionary for task drop result filling  \n",
    "        self.dropped_task.append([])      \n",
    "        if sum(self.tasks[t].values())/self.cpu < 0.4*T: # convert bits into cpu cycles per second\n",
    "            for key,_ in self.tasks[t].items():\n",
    "                self.drop[t][key] = False        \n",
    "        else:            \n",
    "            for key,_ in self.tasks[t].items():  # set drop value for UE to False before setting the drop value to inidividually in the while loop\n",
    "                self.drop[t][key] = False\n",
    "            while sum(self.tasks[t].values())/self.cpu > T : # start removing the minimum size task one by one\n",
    "                if  len(self.tasks[t]) == 0:\n",
    "                    break\n",
    "                min_key = min(zip(self.tasks[t].values(),self.tasks[t].keys()))[1]\n",
    "                self.dropped_task[t].append([min(zip(self.tasks[t].values(),self.tasks[t].keys()))]) \n",
    "                del self.tasks[t][min_key] # drop the task with minimum task size\n",
    "                self.drop[t][min_key] = True\n",
    "\n",
    "        for ue in self.UEs_asso: # update the drop values in UE counter\n",
    "            ue.drop.append(self.drop[t][ue.ID])\n",
    "\n",
    "\n",
    "\n",
    "    def create_first_channel_gains(self):        \n",
    "        self.channelGains = [dict(zip([ue.ID for ue in self.UEs_asso], np.random.choice(np.array(list(self.possible_channel_gains.values())),size =len(self.UEs_asso))))] # a list of channel\n",
    "        # gains for each UE. The list is appended at every time slot by calculating the channel gains using transistion probability matrix.\n",
    "\n",
    "\n",
    "    def generate_channel_gains(self):\n",
    "        # generate channel gains for next slot using transition probability matrix and a set of six possible channel gains\n",
    "        # uses locations of transition matrix for channel gain estimating   \n",
    "        if t == 0:\n",
    "            self.create_first_channel_gains()            \n",
    "        else:    \n",
    "            temp ={}\n",
    "            for key,val in self.channelGains[t-1].items():            \n",
    "                next_channel_no = np.random.choice(self.transition_matrix.columns,p=self.transition_matrix.loc[self.reverse_channel_gain_dict[val]]) #  self.reverse_channel_gain_dict[val] gives channel number\n",
    "                temp[key] = self.possible_channel_gains[next_channel_no]          \n",
    "            self.channelGains.append(temp) \n",
    "     \n",
    "    def calculate_tx_rate(self):\n",
    "        # calculates the transmission rate for a given slot between UE and AP, based on channel gain, alpha, power level and interference from other UEs.\n",
    "        # the AP's tx rate list is appended with the dictionary containing ues' id and corresponding tx rate.\n",
    "        temp = {}\n",
    "        for ue in self.UEs_asso:\n",
    "            temp[ue.ID] = self.B/self.v *np.log2(1+self.channelGains[t][ue.ID]*ue.alpha[t]*ue.P\\\n",
    "                /(self.noise + sum([self.channelGains[t][other_ue.ID]*other_ue.alpha[t]*other_ue.P for other_ue in self.UEs_asso if other_ue.ID != ue.ID ])))\n",
    "            ue.tx_rate.append(temp[ue.ID]) # also copy the tx rate into UE's attribute\n",
    "\n",
    "        self.tx_rate.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(ray.rllib.env.multi_agent_env.MultiAgentEnv):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.UE1 = UE(ID='a')\n",
    "        self.UE2 = UE(ID='b')\n",
    "        self.UE3 = UE(ID='c')\n",
    "        self.AP = FAP(ID='AP')\n",
    "        self._agent_ids = ['a','b','c']\n",
    "        #self._agent_ids = set()\n",
    "        self.AP.UEs_asso = [self.UE1,self.UE2,self.UE3]\n",
    "        self.action_space = gym.spaces.Box(low= np.array([0]), high = np.array([1]), shape = (1,), dtype = np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low= np.array([0]), high = np.array([10]), shape = (1,), dtype = np.float32 )\n",
    "        self.time_step_limit = EPISODE_LENGTH\n",
    "        self.done = False\n",
    "        #self.reset()\n",
    "\n",
    "    def observation_space_sample(self):\n",
    "        self.AP.generate_channel_gains()\n",
    "        for key,val in self.AP.channelGains[-1].items(): # converting values in channel gain dictionary to an array\n",
    "            self.AP.channelGains[-1][key] = np.array([val])\n",
    "        return self.AP.channelGains[-1]\n",
    "\n",
    "    def action_space_sample(self):\n",
    "        return {'a':np.array(np.random.rand()),'b':np.array(np.random.rand()),'c':np.array(np.random.rand())}\n",
    "\n",
    "\n",
    "        \n",
    "    def step(self,action = {'a':np.array(np.random.rand()),'b':np.array(np.random.rand()),'c':np.array(np.random.rand())}):\n",
    "        # currently we are considering only F-AP mode and local processing mode. Hence action for a UE consists of selecting the Power fraction level 'alpha' for offloading a\n",
    "\n",
    "        self.timesteps += 1\n",
    "        for ue in self.AP.UEs_asso:\n",
    "            ue.generate_task()\n",
    "            #ue.create_alpha()        \n",
    "        for id ,alpha in action.items(): # Now to take the respective alpha values for each UE an updaate the alpha attribute of each UE and corresponding local computational capacity\n",
    "            for ue in self.AP.UEs_asso:\n",
    "                if ue.ID == id:\n",
    "                    ue.alpha.append(alpha)\n",
    "                    ue.f.append(((1-ue.alpha[-1])*ue.P/ue.kappa)**(1/3))       \n",
    "            \n",
    "        \n",
    "        self.AP.calculate_tx_rate()\n",
    "        self.AP.get_offloading_request(self.UE1.send_offloading_tasks(),self.UE2.send_offloading_tasks(),self.UE3.send_offloading_tasks())\n",
    "        self.AP.generate_channel_gains()  # try to get channel gains for next state\n",
    "        # The rewards, dones and infos will now be dictionary containing \n",
    "        reward = {}\n",
    "        \n",
    "        for ue in self.AP.UEs_asso:\n",
    "            ue.calculate_utility()\n",
    "            temp = {ue.ID: ue.U[-1]}\n",
    "            reward.update(temp)       \n",
    "\n",
    "        \n",
    "        if self.timesteps >=self.time_step_limit:\n",
    "            self.done = True\n",
    "        \n",
    "        #update dones\n",
    "        dones = {}\n",
    "        infos = {}\n",
    "        for ue in self.AP.UEs_asso:\n",
    "            temp1 = {ue.ID: self.done}\n",
    "            temp2 = {ue.ID: {}}\n",
    "            dones.update(temp1)\n",
    "            infos.update(temp2)\n",
    "        \n",
    "        dones.update({\"__all__\":self.done}) # an extra __all__ key value is required in dones dictionary in multi-agent RL environment.\n",
    "\n",
    "        for key,val in self.AP.channelGains[-1].items(): # converting values in channel gain dictionary to an array\n",
    "            self.AP.channelGains[-1][key] = np.array([val])\n",
    "        \n",
    "\n",
    "        return self.AP.channelGains[-1], reward ,dones,infos #[ue.U[-1] for ue  in self.AP.UEs_asso]\n",
    "        # Let us assume that UEs know that the channel gain at the start of time slot is known to the UE.\n",
    "        # Then state is given as channel gain of the UE . Can the UEs coordinate to decide optimal power allocation strategy so as to maximize the utility.\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.UE1 = UE(ID='a')\n",
    "        self.UE2 = UE(ID='b')\n",
    "        self.UE3 = UE(ID='c')\n",
    "        self.AP = FAP(ID='AP')\n",
    "        self.AP.UEs_asso = [self.UE1,self.UE2,self.UE3]\n",
    "        self.timesteps = 0\n",
    "        t=0\n",
    "        self.AP.generate_channel_gains()\n",
    "        for key,val in self.AP.channelGains[-1].items(): # converting values in channel gain dictionary to an array\n",
    "            self.AP.channelGains[-1][key] = np.array([val])\n",
    "        return self.AP.channelGains[-1]\n",
    "    \n",
    "\n",
    "    def render(self):\n",
    "        print(\"_\"*100)\n",
    "        print(f\"UE Tx rates = {self.AP.tx_rate[-1]}\")\n",
    "        print(f\"UEs task drops = {self.AP.dropped_task[-1]}\")\n",
    "        print(\"_\"*100)\n",
    "        \n",
    "config={}       \n",
    "env = Environment(config) # a config dictionary needs to be passed as per RLlib's specification.\n",
    "\n",
    "\n",
    "\n",
    "def env_creator(_):\n",
    "    return Environment(config)\n",
    "\n",
    "env_name = \"OffloadingEnv\"\n",
    "register_env(env_name, env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A configuration dictionary for the environment\n",
    "config = {\"env\": env_name,\n",
    "    # !PyTorch users!\n",
    "    \"framework\": \"torch\",  # If users have chosen to install torch instead of tf.\n",
    "\n",
    "    \"create_env_on_driver\": True,\n",
    "    \"disable_env_checking\":True,\n",
    "    \"horizon\":EPISODE_LENGTH\n",
    "}\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": None,\n",
    "        \"policy_mapping_fn\": None,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rllib_trainer = SACTrainer(config=config) #TD3Trainer from ddpg also works\n",
    "\n",
    "mean_reward_SAC = []\n",
    "for _ in range(20):   \n",
    "    #rllib_trainer.train()\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")\n",
    "    mean_reward_SAC.append(results['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rllib_trainer_TD3 = TD3Trainer(config=config) #TD3Trainer from ddpg also works\n",
    "rllib_trainer_TD3\n",
    "mean_reward_TD3 = []\n",
    "for _ in range(20):   \n",
    "    #rllib_trainer.train()\n",
    "    results = rllib_trainer_TD3.train()\n",
    "    print(f\"Iteration={rllib_trainer_TD3.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")\n",
    "    mean_reward_TD3.append(results['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,21),mean_reward_SAC,'-m*',np.arange(1,21),mean_reward_TD3,'-kd')\n",
    "plt.xlabel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "out = Output()\n",
    "display.display(out)\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "    env = Environment(config)\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        a1 = rllib_trainer_TD3.compute_actions(obs)['a']\n",
    "        a2 = rllib_trainer_TD3.compute_actions(obs)['b']\n",
    "        a3 = rllib_trainer_TD3.compute_actions(obs)['c']\n",
    "        obs, rewards, dones, _ = env.step({\"a\": a1, \"b\": a2, 'c':a3})\n",
    "        out.clear_output(wait=True)\n",
    "        time.sleep(0.08)\n",
    "        env.render()\n",
    "        if dones['__all__']:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rllib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b7848aad2d913ee78c22c894962d1179a5aa51760fd087e829e65f35b4e5df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
